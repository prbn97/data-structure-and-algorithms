> by [Pavan Katepalli]((https://github.com/pavankat))

> Edited by [Nick Bartlett](https://github.com/tteltrab), and [Nick West](https://github.com/njwest).

![o(n)](/imgs/most-of-them.png)



Big-O | computations for 10 things | computations for 100 things
------| ---- | -----------
O(1)        |   1                         |     1
O(log(n))   |   3                         |     7
O(n)        |  10                         |   100
O(n log(n)) |  30                         |   700
O(n^2)      | 100                         | 10000
O(n^3)      | 1000                         | 1000000
O(2^n)      | 1024                         | 2^100
O(n!)      | 3628800                         | 100! -> mathematically this is the product of (100 * 99 * 98...)

# Dive into O(1)


#### Example 2.1

```js
function returnItem(item){
	return item;
}
```

`returnItem` is a pointless function, but bear with me.

```js
returnItem(2);
```

`returnItem`'s Big O is constant time. No matter what we pass to `returnItem`, the algorithm will go through one unit of work.

The "complexity" of this function is `O(1)`.

If you want to graph `O(1)` then you would set y equal to 1 and graph it.

**y = 1**

![o(1)](/imgs/o_1__plot.png)

Notice that the further right of the horizontal axis (x axis) you go, the vertical axis (y axis) stays the same.

## Dive into O(n)

#### Example 2.2
```js
function itemInList(check, list){
	for (var i = 0; i < list.length; i++){
		if (list[i] === check) return true;
	}
	return false;
}
```

This will run pretty quick:

```js
itemInList(2, [1,2,3]);
```

The "complexity" of `itemInList` is `O(n)`.

This means that it's a linear graph.

For `itemInList`, if the length of the array is 3; **worst case** it'll take 3 units of work.

Sure, in the best case it'll take 1 unit of work, but Big O Notation isn't about the best case scenario, it's about the **worst case scenario**.

If you want to graph `O(n)` then you would replace the `n` with a `x` and set it equal to a `y`.

**y = x**

![o(n)](/imgs/o_n__plot.png)

Notice that the further right of the horizontal axis (x axis) you go, the vertical axis (y axis) goes up too.

## Dive into O(n^2)

#### Example 2.3
```js
function allCombos(list){
	var results = [];
	for (var i = 0; i < results.length; i++){
		for (var j = 0; j < results.length; j++){
			results.push([i, j]);
		}
	}
}
```

If we do `allCombos([1,2,3])` we'd get back `[(1,1) (1,2), (1,3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]`.

The "complexity" of `allCombos` is `O(n^2)`.

The length of the list argument of `allCombos` is the `n` in `O(n^2)`.

allCombos([1]) -> [[1,1]]. One unit of work.  1^2 = 1
allCombos([1,2]) -> [[1,1], [1,2], [2,1], [2,2]]. Four units of work. 2^2 = 4

So n * n is n^2.

## Comparison of O(1), O(n), O(n^2)

Notice that the further right of the horizontal axis (x axis) you go, the vertical axis (y axis) goes up fastest for `O(n^2)`, slower for `O(n)` and constant for `O(1)`.

This means that `O(n^2)` runs slower than `O(n)`, which runs slower than `O(1)`.

![comparison](/imgs/runtime_comparison.png)

## O(1) vs O(n) vs O(n^2) explained without code

#### O(1)
Consider single-digit addition with a pencil and paper. The kind you learned when you were young.

```
5 + 5 = 10

3 + 3 = 6

2 + 2 = 4

2 + 3 = 5

6 + 7 = 13
```

Each of those different problems took the same # of operations to complete (or the same amount of units of work to complete). You take one number and add it to another. Done.

Because it's always the same units of work to complete, no matter what the problem, the Big O is constant, this is an example of `O(1)`.

#### O(n)

Consider multi-digit addition with a pencil and paper.

```
55 + 72 = 127

455 + 322 = 777

1234 + 4447 = 5681

4999 + 56 = 5055
```

Notice how the number of operations (or the amount of units of work to complete) goes up as the number of digits go up.

The number of operations directly correlate (are one to one) with the number of digits of the biggest number being added.

This would take worst case `O(n)` units of work.

#### O(n^2)

Now, consider multi-digit multiplication with pencil and paper.

```
55538 * 92338 = 5128267844
```

This is much harder to do than the previous two asks.

Each digit of the bottom number has to be multiplied by each digit in the top number.

If you were multiplying 100 digit numbers by each other then it would take 10,000 mathematical operations to complete (units of work to complete).

This would take worst case `O(n^2)` units of work to complete.

## Dive into O(log(n))

O(1) < O(log(n)) < O(n) < O(n^2)

![o(log n)](/imgs/o_logn.gif)

#### What does an algorithm look like that has a Big O of O(log(n))?

The choice of the next element on which to perform some action is one of several possibilities, and only one will need to be chosen.

#### Ex. Looking up people in a phone book is O(log(n))

You don't need to check every person in the phone book to find the right one; instead, you can simply divide-and-conquer, and you only need to explore a tiny fraction of the entire space before you eventually find someone's phone number.

Of course, a bigger phone book will still take you a longer time, but it won't grow as quickly as the proportional increase in the additional size.

#### Ex. an algorithm that has a Big O of O(log(n))

```js
function twoDivides(x){
	var count = 0;
	while (parseInt(x) > 1) {
		x = x / 2;
		count = count + 1;
	}
	return count;
}
```

##### Calculating the Big O of algorithm above

**without math**

Often you don't need math to figure out what the Big-O of an algorithm is. You can simply use your intuition.

You look at how many units of work the algorithm has to do as the input grows and match that up to the correct Big O.

Not counting the return or the variable declaration:  
twoDivides(2) = 1. The operations for each step of the loop would be x = 2/1 (for the division) & count = 0 + 1 (for the count); so 2 total.  
twoDivides(4) = 2. The operations would be `x = 4/2` & `count = 0 + 1`, `2/1` & `1 + 1`; so 4 total.  
twoDivides(8) = 3. The operations would be `x = 8/2` & `count = 0 + 1`, `4/2` & `1 + 1`, `2/2` & `2 + 1`; so 6 total.  
twoDivides(16) = 4. The operations would be `x = 16/2` & `count = 0 + 1`, `8/2` & `1 + 1`, `4/2` & `2 + 1`, `2/2` & `3 + 1`, so 8 total.  
twoDivides(32) = 5. The operations would be `x = 32/2` & `count = 0 + 1`, `16/2` & `1 + 1`, `8/2` & `2 + 1`, `4/2` & `3 + 1`, `2/2` & `4 + 1`; so 10 total.  

The "complexity" of twoDivides is `O(log(n))`.  

n    |  operations
-----|---------------
2    |  2
4    |  4
8    |  6
16   |  8
32   |  10
...  | ...
n    | 2 * log(n)

`log(n)` here essentially means "the number of times we can divide `n` by 2".

**Side note:** When writing Big O Notation, the leading "2" would be ignored - it doesn't change significantly the asymptotic behavior of the function for large values of `n`. Thus, we can see `O(2 * log(n))` is equivalent to `O(log(n))`. In general, when writing Big O notation you only care about the most significant portion of complexity (even `2n^2 + 2n` would be written as `O(n^2)`.
    
In this case (`log(n)`), The size of the number is the `n`. We can see that the number of operations isn't constant, but doesn't grow linearly (and grows more slowly as the `n` increases).

**general case, with math** 

Iteration |   x
----------|--------
0         |  x (this is the same thing as x/1)
1         |  x/2
2         |  x/4
...       |  ...
k         |  x/2^k 

2^k = x → Applying log to both sides → k = log(x)

log(2^k) = log(x)

k*log(2) = log(x)

k = log(x)/log(2)

k approximately equals log(x)

## Dive into O(n log n)

O(1) < O(log(n)) < O(n) < O(n log(n)) < O(n^2)

```js
// assume that n is an integer
function nlogn(n){
	var results = [];
	for (var i = 0; i < n; i++){ // this loop is executed n times, so O(n)
	    for (var j = n; j > 0; j = parseInt(j/2)){ // this loop is executed log(n) times, so O(logn)
	    	results.push(j);
	    }
	}
	return results;
}
```

For adjacent for loops you would add the runtimes together, e.g. `O(n + m)`. For nested for loops, you multiply them, e.g. `O(n*m)`, or in this case `O(nlogn)`.

This would result in 

```
nlogn(3)
[3, 1, 3, 1, 3, 1]
```

```
nlogn(4)
[4, 2, 1, 4, 2, 1, 4, 2, 1, 4, 2, 1]
```

## Dive into O(2^n)

Algorithms with a Big O of 2^n are usually recursive.

```js
// assume number is an integer
function fib(number) {
 if (number <= 1) return number;
 return fib(number - 2) + fib(number - 1);
}
```

`O(2^n)` occurs when a problem of size `n` requires solving two smaller problems of size `n-1` (in fibonacci this is close to true, it's just two problems one of size `n-1` and the other of size `n-2`. In essence, you're doubling the number of problems you need to solve every time n increases.

Assume our algorithm takes two operations, and that solving a problem of size `n` requires solving two problems of size `n-1`. Then, the number of operations for increasing values of `n` are:

n   |  ops(n)
--- | ---
1   | 2 
2   | 4 = 2 + 2 = ops(2-1) + ops(2-1) = 2(2) = 2^2
3   | 8 = 4 + 4 = ops(3-1) + ops(3-1) = 2(4) = 2(2^2)= 2^3
4   | 16 = 8 + 8 = ops(4-1) + ops(4-1) = 2(8) = 2(2^3) = 2^4
...
k   | (k-1) + (k-1) = 2(k-1) = 2((k-2) + (k-2)) = 4(k-2) = 8(k-3) = ... = 2^(k-1)(2) = 2^k

## Dive into O(n!)

Any algorithm that calculates all permutations of a given array is `O(n!)`. Factorial is the number you get if you multiply every number from 1 to `n`.

Imagine you have an array of words, and you want to return all possible combinations of those words.

So given
```
['apple', 'bear', 'limp bizkit']
```

The algorithm would return an array of 6 arrays. Like this:
```
[
	['apple', 'bear', 'limp bizkit'],
	['apple', 'limp bizkit', 'bear'],
	['bear', 'limp bizkit', 'apple'],
	['bear', 'apple', 'limp bizkit'],
	['limp bizkit', 'bear', 'apple'],
	['limp bizkit', 'apple', 'bear'],
]
```
Writing an algorithm that would do that would be `O(n!)`. `n` here is the length of the array, so 3! = 3 * 2 * 1 = 6. 

Another example:

```js
// assume n is an integer
function nFactorial(n) {
  for (var i = 0; i < n; i++) {
    return nFactorial(n - 1);
  }
}
```

This runs the `nFactorial` function `n-1` times for an input `n`. So, you get `n*nFactorial(n-1)`.

`n*f(n-1) = n*(n-1)*f(n-2) = ... = n*(n-1)*(n-2)*...*1*f(1) = n!`.